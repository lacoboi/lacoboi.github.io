---
layout:     post
title:      卷积神经网络设计技巧
subtitle:   
date:       2019-01-04
author:     BY Lacoboi
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - CNN
---

# 卷积神经网络设计技巧

## 1x1 卷积核的作用

1x1 的卷积核可以实现信息的通道整合和交互，具有降低/提升特征维度的能力。

## 卷积核的大小如何选择

早期经典网络受限于计算资源，无法叠加的很深，所有利用大的卷积核以获取更大的感受野；但是大的卷积核会导致计算量大幅增加，训练过程缓慢，不利于训练更深的模型。后面的网络选择多个小的卷积核叠加，获取相同大感受野的同时，能够降低参数量和计算量。

## 如何减小卷积层的计算量

- 使用池化操作，在卷积层之前降低特征图分辨率
- 使用堆叠的小卷积核代替大的卷积核
- 使用深度可分离卷积，将原始的 $K \times K \times C$ 的卷积核分为 $K \times K \ 1$ 和 $1 \times 1 \times C$ 两个部分
- 应用 $1 \times 1$ 的卷积进行降维

## 使用宽卷积

所谓宽卷积就是卷积操作时填充方式为 `same`，窄卷积表示填充方式为 `valid`。前者使用填充0的方式对卷积核不能整除时的特征图进行补全，使得卷积层输出维度和输入维度一致；后者不进行任何填充，当边缘位置不足时，会对边缘信息进行舍弃。所以使用宽卷积能够有效保留原始输入特征图的边界特征信息。

## 转置卷积和棋盘效应

当利用反卷积（转置卷积）做图像生成或者上采样的时候或许我们会观察到我们生成的图片会出现一些奇怪的棋盘图案或者说你感觉到你生成的图片有颗粒感

这是因为卷积核的大小不能被步长整除导致的，（如卷积核大小是3，步长大小是1），相当与特征图会有一定程度的叠加。

解决方法：使用上采样（如双线性差值）+卷积代替转置卷积。

## 提升卷积神经网络的泛化性能

- 使用更多的数据
- 使用大的batch-size
- 数据增强
- 修改损失函数，如Focal loss，GHM loss，IOU loss
- 修改网络，小的网络会导致欠拟合、大的网络能提升泛化性能
- 正则化、BN、Dropout



