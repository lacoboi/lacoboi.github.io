---
layout:     post
title:      cuda流和并发
subtitle:   
date:       2020-09-05
author:     BY Lacoboi
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - CUDA
---

# 6.流和并发

## 6.1 流和事件概述

`CUDA流` 是一系列异步CUDA操作的封装，这些操作包括：

- 主机与设备之间的数据传输，如 cudaMemcpy等
- 核函数启动
- 其他由主机端发出的在设备上执行的命令，如cudaMalloc等

流能够封装这些异步的操作，允许这些操作在流中排队，并保持顺序执行，保证其在前面所有的操作启动之后再启动。流中的操作相对于主机来说总是异步的，CUDA运行时决定何时可以在设备上执行操作，我们需要做的就是控制这些操作在其出结果之前，不启动需要调用这个结果的操作。一个流中的不同操作有着严格的启动顺序，但是不同流之间是没有限制的，多个流同时启动多个内核，就能形成网格级的并行。

由于 `CUDA流` 和主机总是异步的，所以流中操作的执行并不影响主机运行其他指令，所以可以隐藏这些操作的开销。

因为硬件资源的限制，所有编程模型和硬件的实际执行是有差异的，因为PCIE总线和SM的数量是有限的，这些资源会限制并发数。

### 6.1.1 CUDA流

所有的 `CUDA` 操作都是在流中执行的，如果没有显示指定，则操作在系统默认的流中进行，所以 `CUDA` 流可以分为：

- 隐式声明的流，也叫空流、默认流
- 显示声明的流，也叫非空流

空流是没有办法管理的，因为连名字也没有，因此要想控制流，必须建立非空流，基于流的异步内核启动和数据传输支持以下类型的粗粒度并发：

- 重叠主机和设备计算
- 重叠主机计算和主机设备数据传输
- 重叠设备计算和主机设备数据传输
- 并发设备计算（多个设备）

主机和设备数据传输有两种不同的形式，一个是同步模式，另一种是异步模式，函数原型分别是，默认接收的是空流：
```c
cudaError_t cudaMemcpy(void* dst, const void* src, size_t count,
cudaMemcpyKind kind, cudaStream_t stream = 0);  // 同步形式
cudaError_t cudaMemcpyAsync(void* dst, const void* src, size_t count,
cudaMemcpyKind kind, cudaStream_t stream = 0);  // 异步形式
```

需要注意的是，在执行异步数据传输时，主机端的内存必须是固页内存，通过如下方式可以分配固页内存：
```c
cudaError_t cudaMallocHost(void **ptr, size_t size);
cudaError_t cudaHostAlloc(void **pHost, size_t size, unsigned int flags);
```

`CUDA流` 的声明、创建和资源回收如下：
```c
cudaStream_t stream;   // 声明一个流
cudaError_t cudaStreamCreate(cudaStream_t* pStream); // 创建流函数原型
cudaError_t cudaStreamDestroy(cudaStream_t stream);  // 资源回收
```

使用 `cudaStreamDestroy` 函数回收流资源时，如果流正在执行，那么不会立刻停止流，而是等待流执行完成后，立刻回收流中的资源。

可以通过下面两条指令查看流的执行状态，
```c
cudaError_t cudaStreamSynchronize(cudaStream_t stream);
cudaError_t cudaStreamQuery(cudaStream_t stream);
```

`cudaStreamSynchronize` 函数会阻塞主机，直到流完成；而 `cudaStreamQuery` 不会阻塞流的执行，而是返回 `cudaSuccess` 表示流执行完了，返回 `cudaErrorNotReady` 表示流正在执行中。
 

### 6.1.2 流的调度

从编程模型看，所有的流可以同时执行，但是硬件资源是有限的，接下来将从硬件调度的角度理解流的并发。

#### 6.1.2.1 虚假的依赖关系

对于 `Fermi` 架构支持16路并发执行，但是所有的流都是在单一硬件上执行，`Fermi` 架构只有一个硬件工作队列，当要执行某个网格的时候，CUDA会检测依赖关系，如果依赖与其他结果，则会等待，单一流水线可能会导致虚假依赖关系。

   ![虚假依赖](/img/post_images/cuda/6-2.png)

#### 6.1.2.2 Hyper-Q技术

有多个工作硬件工作队列可以从根本上解决虚假依赖关系，`Hyper-Q` 技术支持32个硬件工作队列同时执行多个流，可以实现所有流的并发，最小化虚假依赖。

   ![Hyper-Q](/img/post_images/cuda/6-3.png)

### 6.1.3 流的优先级

3.5以上的设备可以支持给流分配优先级，可以通过以下函数创建带优先级的流：
```c
cudaError_t cudaStreamCreateWithPriority(cudaStream_t* pStream, 
    unsigned int flags, int priority);
```

### 6.1.4 CUDA事件

`CUDA` 事件本质上是一个标记，可以用来
- 同步流的执行
- 监控流中操作的进展

流中的任意点都可以通过API函数插入事件以及查询事件完成的函数，流中某一事件前的所有操作都完成了，才能触发该事件完成。

#### 6.1.4.1 创建和销毁

可以通过下面的API函数声明、创建和销毁 `CUDA事件`：
```c
cudaEvent_t event; // 声明
cudaError_t cudaEventCreate(cudaEvent_t* event); // 分配资源
cudaError_t cudaEventDestroy(cudaEvent_t event); // 资源回收
```

如果事件回收指令执行的时候，事件还没有触发，则指令会立即完成，等到事件完成以后，资源会立马被回收。

#### 6.1.4.2 记录事件和计算运行时间

CUDA事件的一个主要作用就是记录事件之间的时间间隔，事件可以通过以下指令添加到CUDA流中：
```c
cudaError_t cudaEventRecord(cudaEvent_t event, cudaStream_t stream = 0);
```

在流中事件的主要作用就是等待前面的操作完成，可以通过下面两个API函数查询事件是否触发：
```c
cudaError_t cudaEventSynchronize(cudaEvent_t event);  // 同步版本，阻塞主机线程
cudaError_t cudaEventQuery(cudaEvent_t event);  // 异步版本，不会阻塞主机线程
```

可以通过下面的API函数记录两个事件之间的时间间隔：
```c
cudaError_t cudaEventElapsedTime(float* ms, cudaEvent_t start, cudaEvent_t stop);
```

下面的程序是一个简单的记录事件时间间隔的代码：
```c
// create two events
cudaEvent_t start, stop;
cudaEventCreate(&amp;start);
cudaEventCreate(&amp;stop);
// record start event on the default stream
cudaEventRecord(start);
// execute kernel
kernel<<<grid, block>>>(arguments);
// record stop event on the default stream
cudaEventRecord(stop);
// wait until the stop event completes
cudaEventSynchronize(stop);
// calculate the elapsed time between two events
float time;
cudaEventElapsedTime(&time, start, stop);
// clean up the two events
cudaEventDestroy(start);
cudaEventDestroy(stop);
```

### 6.1.5 流同步

前面提到，流也可以分为：
- 同步流（空流/默认流）
- 异步流（非空流）

同时，流又可以分为：
- 阻塞流
- 非阻塞流

同步流中的部分操作会造成主机阻塞，异步流通常不会阻塞主机，但是会被空流中的操作阻塞，如果一个非空流声明为非阻塞的流，那么没有操作能阻塞它，但是如果声明为阻塞流，则会被空流阻塞。



#### 6.1.5.1 阻塞流和非阻塞流

默认用 `cudaStreamCreate` 创建的流是阻塞流，可以用下列函数创建一个非阻塞流：
```c
cudaError_t cudaStreamCreateWithFlags(cudaStream_t* pStream, 
    unsigned int flags);
// 其中第二个参数可以选择创建阻塞流或者非阻塞流
// cudaStreamDefault;// 默认阻塞流
// cudaStreamNonBlocking: //非阻塞流，对空流的阻塞行为失效。
```

对于下面的代码，核函数kernel_1和kernel_3在非空流中执行，kernel_2在默认的空流中执行，如果kernel_1和kernel_3被声明为阻塞流，则kernel_1启动并执行后，将控制权交给主机，主机启动kernel_2，但是kernel_2不会马上执行，而是会等待kernel_1执行完成之后再执行，kernel_2执行时，将控制权交给主机，主机继续启动kernel_3，但是同样，kernel_3需要等待kernel_2执行完成之后才能执行，也就是说这三个核函数是串行执行的；而如果kernel_1和kernel_3被声明为非阻塞流，那么这三个和函数就可以并行执行。
```c
kernel_1<<<1, 1, 0, stream_1>>>();
kernel_2<<<1, 1>>>();
kernel_3<<<1, 1, 0, stream_2>>>();
```

#### 6.1.5.2 隐式同步

隐式同步指令：
- cudaMemcpy

隐式同步会造成性能下降，所谓同步就是阻塞的意思，隐式同步通常出现在内存操作上，如
- 锁页主机内存分配
- 设备内存分配
- 设备内存初始化

显示同步指令：
- cudaDeviceSynchronize;
- cudaStreamSynchronize;
- cudaEventSynchronize;

#### 6.1.5.3 显式同步

显示同步主要用于：
- 同步设备
- 同步流
- 同步流中的事件
- 使用事件跨流同步

使用 `cudaDeviceSynchronize` 可以阻塞主机线程，应尽量避免使用，防止影响主机执行。
```c
cudaError_t cudaDeviceSynchronize(void);
```

使用下面两个函数可以同步流：
```c
cudaError_t cudaStreamSynchronize(cudaStream_t stream); // 阻塞型
cudaError_t cudaStreamQuery(cudaStream_t stream);       // 非阻塞型
```

使用下面两个函数可以同步事件：
```c
cudaError_t cudaEventSynchronize(cudaEvent_t event);    // 阻塞型
cudaError_t cudaEventQuery(cudaEvent_t event);          // 非阻塞型
```

使用下面的函数可以利用事件实现流间的同步：
```c
cudaError_t cudaStreamWaitEvent(cudaStream_t stream, cudaEvent_t event);
```

#### 6.1.5.4 可配置事件

CDUA提供了一种控制事件行为和性能的函数：

```c
cudaError_t cudaEventCreateWithFlags(cudaEvent_t* event, 
    unsigned int flags);
```

其中的参数可以是：
- cudaEventDefault  // 默认参数
- cudaEventBlockingSync     // 创建新线程用于查询事件同步
- cudaEventDisableTiming    // 表示不用于计时，可以提升性能
- cudaEventInterprocess     // 表示可能会用于进程的事件


## 6.2 并发内核执行
前面说到了流、事件和同步的概念，接下来的几个例子，介绍并发内核的几个基本问题，包括不限于以下几个方面：
- 使用深度优先或者广度优先方法的调度工作
- 调整硬件工作队列
- 在Kepler设备和Fermi设备上避免虚假的依赖关系
- 检查默认流的阻塞行为
- 在非默认流之间添加依赖关系
- 检查资源使用是如何影响并发的

### 6.2.1 非空流中的并发内核
创建不同的非空流，每个非空流中插入4个核函数，观察核函数的执行。
```c
#include <cuda_runtime.h>
#include <stdio.h>
#include "freshman.h"

#define N 300000
__global__ void kernel_1()
{
    double sum=0.0;
    for(int i=0;i<N;i++)
        sum=sum+tan(0.1)*tan(0.1);
}
__global__ void kernel_2()
{
    double sum=0.0;
    for(int i=0;i<N;i++)
        sum=sum+tan(0.1)*tan(0.1);
}
__global__ void kernel_3()
{
    double sum=0.0;
    for(int i=0;i<N;i++)
        sum=sum+tan(0.1)*tan(0.1);
}
__global__ void kernel_4()
{
    double sum=0.0;
    for(int i=0;i<N;i++)
        sum=sum+tan(0.1)*tan(0.1);
}
int main()
{
    setenv("CUDA_DEVICE_MAX_CONNECTIONS","32",1);
    int dev = 0;
    cudaSetDevice(dev);
    int n_stream=16;
    cudaStream_t *stream=(cudaStream_t*)malloc(n_stream*sizeof(cudaStream_t));
    for(int i=0;i<n_stream;i++)
    {
        cudaStreamCreate(&stream[i]);
    }
    dim3 block(1);
    dim3 grid(1);
    cudaEvent_t start,stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(start,0);
    for(int i=0;i<n_stream;i++)
    {
        kernel_1<<<grid,block,0,stream[i]>>>();
        kernel_2<<<grid,block,0,stream[i]>>>();
        kernel_3<<<grid,block,0,stream[i]>>>();
        kernel_4<<<grid,block,0,stream[i]>>>();
    }
    cudaEventRecord(stop,0);
    CHECK(cudaEventSynchronize(stop));
    float elapsed_time;
    cudaEventElapsedTime(&elapsed_time,start,stop);
    printf("elapsed time:%f ms\n",elapsed_time);

    for(int i=0;i<n_stream;i++)
    {
        cudaStreamDestroy(stream[i]);
    }
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    free(stream);
    CHECK(cudaDeviceReset());
    return 0;
}
```

### 6.2.2 Fermi GPU 上的虚假依赖关系
`Fermi GPU` 只有一个硬件工作队列，使用广度优先的方法组织任务，可以解决虚假依赖的问题。
```c
// dispatch job with breadth first way
for (int i = 0; i < n_streams; i++)
    kernel_1<<<grid, block, 0, streams[i]>>>();
for (int i = 0; i < n_streams; i++)
    kernel_2<<<grid, block, 0, streams[i]>>>();
for (int i = 0; i < n_streams; i++)
    kernel_3<<<grid, block, 0, streams[i]>>>();
for (int i = 0; i < n_streams; i++)
    kernel_4<<<grid, block, 0, streams[i]>>>();
```

### 6.2.3 使用OpenMP的调度操作
`OpenMP` 是一种非常好的并行工具：
```c
omp_set_num_thread(n_stream); // 创建 n_stream 个线程
#pragma omp parallel // 宏指令表示大括号中的代码是每个线程需要执行的代码
    {
        int i=omp_get_thread_num();
        kernel_1<<<grid,block,0,stream[i]>>>();
        kernel_2<<<grid,block,0,stream[i]>>>();
        kernel_3<<<grid,block,0,stream[i]>>>();
        kernel_4<<<grid,block,0,stream[i]>>>();
    }
```

### 6.2.4 用环境变量调整流行为
`Kepler` 支持的最大 `Hyper-Q` 工作队列数是32，但是在默认情况下并不是全部开启，而是被限制成8个，原因是每个工作队列只要开启就会有资源消耗，如果用不到32个可以把资源留给需要的8个队列，修改这个配置的方法是修改主机系统的环境变量。

对于 `Linux` 系统修改方式如下：
```bash
export CUDA_DEVICE_MAX_CONNECTIONS=32
```
## 6.2 并发内核执行

### 6.2.5 GPU资源的并发限制
限制内核并发数量的最根本的还是GPU上面的资源，资源才是性能的极限，性能最高无非是在不考虑算法进化的前提下，资源利用率最高的结果。当每个内核的线程数增加的时候，内核级别的并行数量就会下降。

### 6.2.6 默认流的阻塞行为
默认流（即空流）对于非空流中的阻塞流是有阻塞作用的，对于没有指定流的那些GPU操作指令，默认是在空流上执行的，空流是阻塞流，同时我们声明定义的流如果没有特别指出，声明的也是阻塞流，这个时候，默认流（空流）对非空阻塞流会有阻塞作用。

### 6.2.7 创建流间依赖关系
流之间的虚假依赖关系是需要避免的，而经过我们设计的依赖又可以保证流之间的同步性，避免内存竞争，这时候我们要使用的就是事件这个工具了，换句话说，我们可以让某个特定流等待某个特定的事件，这个事件可以再任何流中，只有此事件完成才能进一步执行等待此事件的流继续执行。

这种事件往往不用于计时，所以可以在生命的时候声明成 cudaEventDisableTiming 的同步事件：
```c
cudaEvent_t * event = (cudaEvent_t *)malloc(n_stream*sizeof(cudaEvent_t));
for(int i=0; i<n_stream; i++)
{
    cudaEventCreateWithFlag(event[i],cudaEventDisableTiming);
}
```
在流中加入指令：
```c
for(int i=0; i < n_stream;i++)
{
    kernel_1<<<grid,block,0,stream[i]>>>();
    kernel_2<<<grid,block,0,stream[i]>>>();
    kernel_3<<<grid,block,0,stream[i]>>>();
    kernel_4<<<grid,block,0,stream[i]>>>();
    cudaEventRecord(event[i],stream[i]);
    cudaStreamWaitEvent(stream[n_stream-1],event[i],0);
}
```
这时候，最后一个流（第5个流）都会等到前面所有流中的事件完成，自己才会完成。



## 6.3 重叠内核执行和数据传输

Fermi架构和Kepler架构下有两个复制引擎队列，也就是数据传输队列，一个从设备到主机，一个从主机到设备。所以读取和写入是不经过同一条队列的，这样的好处就是这两个操作可以重叠完成了，注意，只有方向不同的时候才能数据操作。同向的时候不能进行此操作。

应用程序中，还需要检查数据传输和内核执行之间的关系，分为以下两种：
- 如果内核使用数据A，那么对A进行数据传输必须要安排在内核启动之前，且必须在同一个流中
- 如果内核完全不使用数据A，那么内核执行和数据传输可以位于不同的流中重叠执行。

第二种情况就是重叠内核执行和数据传输的基本做法，当数据传输和内核执行被分配到不同的流中时，CUDA执行的时候默认这是安全的，也就是程序编写者要保证他们之间的依赖关系。

### 6.3.1 使用深度优先调度重叠

```c
    cudaStream_t stream[N_SEGMENT];
    for(int i=0;i < N_SEGMENT;i++)
    {
        CHECK(cudaStreamCreate(stream[i]));
    }
    cudaEvent_t start,stop;
    cudaEventCreate(start);
    cudaEventCreate(stop);
    cudaEventRecord(start,0);
    for(int i=0;i<N_SEGMENT;i++)
    {
        int ioffset=i*iElem;
        CHECK(cudaMemcpyAsync(&a_d[ioffset],&a_h[ioffset],nByte/N_SEGMENT,
        cudaMemcpyHostToDevice,stream[i]));
        CHECK(cudaMemcpyAsync(&b_d[ioffset],&b_h[ioffset],nByte/N_SEGMENT,
        cudaMemcpyHostToDevice,stream[i]));
        sumArraysGPU<<<grid,block,0,stream[i]>>>
        (&a_d[ioffset],&b_d[ioffset],&res_d[ioffset],iElem);
        CHECK(cudaMemcpyAsync(&res_from_gpu_h[ioffset],&res_d[ioffset],
        nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));
    }
    //timer
    CHECK(cudaEventRecord(stop, 0));
    CHECK(cudaEventSynchronize(stop));
```

### 6.3.2 使用广度优先调度重叠
```c
    for(int i=0;i<N_SEGMENT;i++)
    {
        int ioffset=i*iElem;
        CHECK(cudaMemcpyAsync(&a_d[ioffset],&a_h[ioffset],nByte/N_SEGMENT,
        cudaMemcpyHostToDevice,stream[i]));
        CHECK(cudaMemcpyAsync(&b_d[ioffset],&b_h[ioffset],nByte/N_SEGMENT,
        cudaMemcpyHostToDevice,stream[i]));
    }
    for(int i=0;i<N_SEGMENT;i++)
    {
        int ioffset=i*iElem;
        sumArraysGPU<<<grid,block,0,stream[i]>>>
        (&a_d[ioffset],&b_d[ioffset],&res_d[ioffset],iElem);
    }
    for(int i=0;i<N_SEGMENT;i++)
    {
        int ioffset=i*iElem;
        CHECK(cudaMemcpyAsync(&res_from_gpu_h[ioffset],&res_d[ioffset],nByte/N_SEGMENT,
        cudaMemcpyDeviceToHost,stream[i]));
    }
```

## 6.4 重叠GPU和CPU的执行

除了上文说到的重叠数据传输和核函数的同时执行，另一个最主要的问题就是使用GPU的同时CPU也进行计算，这就是我们本文关注的重点。
本文示例过程如下：
- 内核调度到各自的流中
- CPU在等待事件的同时进行计算

```c
for(int i=0;i<N_SEGMENT;i++)
{
    int ioffset=i*iElem;
    CHECK(cudaMemcpyAsync(&a_d[ioffset],&a_h[ioffset],nByte/N_SEGMENT,
    cudaMemcpyHostToDevice,stream[i]));
    CHECK(cudaMemcpyAsync(&b_d[ioffset],&b_h[ioffset],nByte/N_SEGMENT,
    cudaMemcpyHostToDevice,stream[i]));
    sumArraysGPU<<<grid,block,0,stream[i]>>>
    (&a_d[ioffset],&b_d[ioffset],&res_d[ioffset],iElem);
    CHECK(cudaMemcpyAsync(&res_from_gpu_h[ioffset],&res_d[ioffset],
    nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));
}
//timer
CHECK(cudaEventRecord(stop, 0));
int counter=0;
while (cudaEventQuery(stop)==cudaErrorNotReady)
{
    counter++;
}
```
在事件stop执行之前，CPU是一直在工作的，这就达到一种并行的效果，代码中的关键是 `cudaEventQuery(stop)` 是非阻塞的，否则，不能继续 `cpu` 的计算。

## 6.5 流回调
流回调是一种特别的技术，有点像是事件的函数，这个回调函数被放入流中，当其前面的任务都完成了，就会调用这个函数，但是比较特殊的是，在回调函数中，需要遵守下面的规则：
- 回调函数中不可以调用CUDA的API
- 不可以执行同步

流回调函数是应用程序提供的主机函数，使用 `cudaStreamAddCallback` API函数进行注册：
```c
cudaError_t cudaStreamAddCallback(cudaStream_t stream,
    cudaStreamCallback_t callback, void *userData, unsigned int flags);
```

流回调函数的格式：
```c
void CUDART_CB my_callback(cudaStream_t stream, cudaError_t status, void *data) {
    printf("callback from stream %d\n", *((int *)data));
}
```

在流中插入回调函数：
```c
void CUDART_CB my_callback(cudaStream_t stream, cudaError_t status, void *data)
{
    printf("callback from stream %d\n", *((int *)data));
}
int stream_ids[n_streams];

for (int i = 0; i < n_streams; i++)
{
    stream_ids[i] = i;
    kernel_1<<<grid, block, 0, streams[i]>>>();
    kernel_2<<<grid, block, 0, streams[i]>>>();
    kernel_3<<<grid, block, 0, streams[i]>>>();
    kernel_4<<<grid, block, 0, streams[i]>>>();
    CHECK(cudaStreamAddCallback(streams[i], my_callback, 
        (void *)(stream_ids + i), 0));
}
```