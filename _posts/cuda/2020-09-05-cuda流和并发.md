---
layout:     post
title:      cuda流和并发
subtitle:   
date:       2020-09-05
author:     BY Lacoboi
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - CUDA
---

# 6.流和并发


## 6.1 流和事件概述

### 6.1.1 CUDA流

### 6.1.2 流的调度
#### 6.1.2.1 虚假的依赖关系
#### 6.1.2.2 Hyper-Q技术


### 6.1.3 流的优先级


### 6.1.4 CUDA事件
#### 6.1.4.1 创建和销毁
#### 6.1.4.2 记录事件和计算运行时间


### 6.1.5 流同步
#### 6.1.5.1 阻塞流和非阻塞流
#### 6.1.5.2 隐式同步
#### 6.1.5.3 显式同步
#### 6.1.5.4 可配置事件


## 6.2 并发内核执行
前面说到了流、事件和同步的概念，接下来的几个例子，介绍并发内核的几个基本问题，包括不限于以下几个方面：
- 使用深度优先或者广度优先方法的调度工作
- 调整硬件工作队列
- 在Kepler设备和Fermi设备上避免虚假的依赖关系
- 检查默认流的阻塞行为
- 在非默认流之间添加依赖关系
- 检查资源使用是如何影响并发的

### 6.2.1 非空流中的并发内核
创建不同的非空流，每个非空流中插入4个核函数，观察核函数的执行。
```c
#include <cuda_runtime.h>
#include <stdio.h>
#include "freshman.h"

#define N 300000
__global__ void kernel_1()
{
    double sum=0.0;
    for(int i=0;i<N;i++)
        sum=sum+tan(0.1)*tan(0.1);
}
__global__ void kernel_2()
{
    double sum=0.0;
    for(int i=0;i<N;i++)
        sum=sum+tan(0.1)*tan(0.1);
}
__global__ void kernel_3()
{
    double sum=0.0;
    for(int i=0;i<N;i++)
        sum=sum+tan(0.1)*tan(0.1);
}
__global__ void kernel_4()
{
    double sum=0.0;
    for(int i=0;i<N;i++)
        sum=sum+tan(0.1)*tan(0.1);
}
int main()
{
    setenv("CUDA_DEVICE_MAX_CONNECTIONS","32",1);
    int dev = 0;
    cudaSetDevice(dev);
    int n_stream=16;
    cudaStream_t *stream=(cudaStream_t*)malloc(n_stream*sizeof(cudaStream_t));
    for(int i=0;i<n_stream;i++)
    {
        cudaStreamCreate(&stream[i]);
    }
    dim3 block(1);
    dim3 grid(1);
    cudaEvent_t start,stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(start,0);
    for(int i=0;i<n_stream;i++)
    {
        kernel_1<<<grid,block,0,stream[i]>>>();
        kernel_2<<<grid,block,0,stream[i]>>>();
        kernel_3<<<grid,block,0,stream[i]>>>();
        kernel_4<<<grid,block,0,stream[i]>>>();
    }
    cudaEventRecord(stop,0);
    CHECK(cudaEventSynchronize(stop));
    float elapsed_time;
    cudaEventElapsedTime(&elapsed_time,start,stop);
    printf("elapsed time:%f ms\n",elapsed_time);

    for(int i=0;i<n_stream;i++)
    {
        cudaStreamDestroy(stream[i]);
    }
    cudaEventDestroy(start);
    cudaEventDestroy(stop);
    free(stream);
    CHECK(cudaDeviceReset());
    return 0;
}
```

### 6.2.2 Fermi GPU 上的虚假依赖关系
`Fermi GPU` 只有一个硬件工作队列，使用广度优先的方法组织任务，可以解决虚假依赖的问题。
```c
// dispatch job with breadth first way
for (int i = 0; i < n_streams; i++)
    kernel_1<<<grid, block, 0, streams[i]>>>();
for (int i = 0; i < n_streams; i++)
    kernel_2<<<grid, block, 0, streams[i]>>>();
for (int i = 0; i < n_streams; i++)
    kernel_3<<<grid, block, 0, streams[i]>>>();
for (int i = 0; i < n_streams; i++)
    kernel_4<<<grid, block, 0, streams[i]>>>();
```

### 6.2.3 使用OpenMP的调度操作
`OpenMP` 是一种非常好的并行工具：
```c
omp_set_num_thread(n_stream); // 创建 n_stream 个线程
#pragma omp parallel // 宏指令表示大括号中的代码是每个线程需要执行的代码
    {
        int i=omp_get_thread_num();
        kernel_1<<<grid,block,0,stream[i]>>>();
        kernel_2<<<grid,block,0,stream[i]>>>();
        kernel_3<<<grid,block,0,stream[i]>>>();
        kernel_4<<<grid,block,0,stream[i]>>>();
    }
```

### 6.2.4 用环境变量调整流行为
`Kepler` 支持的最大 `Hyper-Q` 工作队列数是32，但是在默认情况下并不是全部开启，而是被限制成8个，原因是每个工作队列只要开启就会有资源消耗，如果用不到32个可以把资源留给需要的8个队列，修改这个配置的方法是修改主机系统的环境变量。

对于 `Linux` 系统修改方式如下：
```bash
export CUDA_DEVICE_MAX_CONNECTIONS=32
```## 6.2 并发内核执行

### 6.2.5 GPU资源的并发限制
限制内核并发数量的最根本的还是GPU上面的资源，资源才是性能的极限，性能最高无非是在不考虑算法进化的前提下，资源利用率最高的结果。当每个内核的线程数增加的时候，内核级别的并行数量就会下降。

### 6.2.6 默认流的阻塞行为
默认流（即空流）对于非空流中的阻塞流是有阻塞作用的，对于没有指定流的那些GPU操作指令，默认是在空流上执行的，空流是阻塞流，同时我们声明定义的流如果没有特别指出，声明的也是阻塞流，这个时候，默认流（空流）对非空阻塞流会有阻塞作用。

### 6.2.7 创建流间依赖关系
流之间的虚假依赖关系是需要避免的，而经过我们设计的依赖又可以保证流之间的同步性，避免内存竞争，这时候我们要使用的就是事件这个工具了，换句话说，我们可以让某个特定流等待某个特定的事件，这个事件可以再任何流中，只有此事件完成才能进一步执行等待此事件的流继续执行。

这种事件往往不用于计时，所以可以在生命的时候声明成 cudaEventDisableTiming 的同步事件：
```c
cudaEvent_t * event=(cudaEvent_t *)malloc(n_stream*sizeof(cudaEvent_t));
for(int i=0; i<n_stream; i++)
{
    cudaEventCreateWithFlag(event[i],cudaEventDisableTiming);
}
```
在流中加入指令：
```c
for(int i=0; i < n_stream;i++)
{
    kernel_1<<<grid,block,0,stream[i]>>>();
    kernel_2<<<grid,block,0,stream[i]>>>();
    kernel_3<<<grid,block,0,stream[i]>>>();
    kernel_4<<<grid,block,0,stream[i]>>>();
    cudaEventRecord(event[i],stream[i]);
    cudaStreamWaitEvent(stream[n_stream-1],event[i],0);
}
```
这时候，最后一个流（第5个流）都会等到前面所有流中的事件完成，自己才会完成。



## 6.3 重叠内核执行和数据传输

Fermi架构和Kepler架构下有两个复制引擎队列，也就是数据传输队列，一个从设备到主机，一个从主机到设备。所以读取和写入是不经过同一条队列的，这样的好处就是这两个操作可以重叠完成了，注意，只有方向不同的时候才能数据操作。同向的时候不能进行此操作。

应用程序中，还需要检查数据传输和内核执行之间的关系，分为以下两种：
- 如果内核使用数据A，那么对A进行数据传输必须要安排在内核启动之前，且必须在同一个流中
- 如果内核完全不使用数据A，那么内核执行和数据传输可以位于不同的流中重叠执行。

第二种情况就是重叠内核执行和数据传输的基本做法，当数据传输和内核执行被分配到不同的流中时，CUDA执行的时候默认这是安全的，也就是程序编写者要保证他们之间的依赖关系。

### 6.3.1 使用深度优先调度重叠

```c
    cudaStream_t stream[N_SEGMENT];
    for(int i=0;i < N_SEGMENT;i++)
    {
        CHECK(cudaStreamCreate(stream[i]));
    }
    cudaEvent_t start,stop;
    cudaEventCreate(start);
    cudaEventCreate(stop);
    cudaEventRecord(start,0);
    for(int i=0;i<N_SEGMENT;i++)
    {
        int ioffset=i*iElem;
        CHECK(cudaMemcpyAsync(&a_d[ioffset],&a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));
        CHECK(cudaMemcpyAsync(&b_d[ioffset],&b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));
        sumArraysGPU<<<grid,block,0,stream[i]>>>(&a_d[ioffset],&b_d[ioffset],&res_d[ioffset],iElem);
        CHECK(cudaMemcpyAsync(&res_from_gpu_h[ioffset],&res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));
    }
    //timer
    CHECK(cudaEventRecord(stop, 0));
    CHECK(cudaEventSynchronize(stop));
```

### 6.3.2 使用广度优先调度重叠
```c
    for(int i=0;i<N_SEGMENT;i++)
    {
        int ioffset=i*iElem;
        CHECK(cudaMemcpyAsync(&a_d[ioffset],&a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));
        CHECK(cudaMemcpyAsync(&b_d[ioffset],&b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));
    }
    for(int i=0;i<N_SEGMENT;i++)
    {
        int ioffset=i*iElem;
        sumArraysGPU<<<grid,block,0,stream[i]>>>(&a_d[ioffset],&b_d[ioffset],&res_d[ioffset],iElem);
    }
    for(int i=0;i<N_SEGMENT;i++)
    {
        int ioffset=i*iElem;
        CHECK(cudaMemcpyAsync(&res_from_gpu_h[ioffset],&res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));
    }
```

## 6.4 重叠GPU和CPU的执行

除了上文说到的重叠数据传输和核函数的同时执行，另一个最主要的问题就是使用GPU的同时CPU也进行计算，这就是我们本文关注的重点。
本文示例过程如下：
- 内核调度到各自的流中
- CPU在等待事件的同时进行计算

```c
for(int i=0;i<N_SEGMENT;i++)
{
    int ioffset=i*iElem;
    CHECK(cudaMemcpyAsync(&a_d[ioffset],&a_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));
    CHECK(cudaMemcpyAsync(&b_d[ioffset],&b_h[ioffset],nByte/N_SEGMENT,cudaMemcpyHostToDevice,stream[i]));
    sumArraysGPU<<<grid,block,0,stream[i]>>>(&a_d[ioffset],&b_d[ioffset],&res_d[ioffset],iElem);
    CHECK(cudaMemcpyAsync(&res_from_gpu_h[ioffset],&res_d[ioffset],nByte/N_SEGMENT,cudaMemcpyDeviceToHost,stream[i]));
}
//timer
CHECK(cudaEventRecord(stop, 0));
int counter=0;
while (cudaEventQuery(stop)==cudaErrorNotReady)
{
    counter++;
}
```
在事件stop执行之前，CPU是一直在工作的，这就达到一种并行的效果，代码中的关键是 `cudaEventQuery(stop)` 是非阻塞的，否则，不能继续 `cpu` 的计算。

## 6.5 流回调
流回调是一种特别的技术，有点像是事件的函数，这个回调函数被放入流中，当其前面的任务都完成了，就会调用这个函数，但是比较特殊的是，在回调函数中，需要遵守下面的规则：
- 回调函数中不可以调用CUDA的API
- 不可以执行同步

流回调函数是应用程序提供的主机函数，使用 `cudaStreamAddCallback` API函数进行注册：
```c
cudaError_t cudaStreamAddCallback(cudaStream_t stream,cudaStreamCallback_t callback, void *userData, unsigned int flags);
```

流回调函数的格式：
```c
void CUDART_CB my_callback(cudaStream_t stream, cudaError_t status, void *data) {
    printf("callback from stream %d\n", *((int *)data));
}
```

在流中插入回调函数：
```c
void CUDART_CB my_callback(cudaStream_t stream, cudaError_t status, void *data)
{
    printf("callback from stream %d\n", *((int *)data));
}
int stream_ids[n_streams];

for (int i = 0; i < n_streams; i++)
{
    stream_ids[i] = i;
    kernel_1<<<grid, block, 0, streams[i]>>>();
    kernel_2<<<grid, block, 0, streams[i]>>>();
    kernel_3<<<grid, block, 0, streams[i]>>>();
    kernel_4<<<grid, block, 0, streams[i]>>>();
    CHECK(cudaStreamAddCallback(streams[i], my_callback, (void *)(stream_ids + i), 0));
}
```