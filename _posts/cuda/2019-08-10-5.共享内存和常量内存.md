---
layout:     post
title:      5.共享内存和常量内存
subtitle:   
date:       2019-08-10
author:     BY Lacoboi
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - CUDA
---

# 5.共享内存和常量内存

学习如何使用共享内存，提升核函数性能。

## 5.1 CUDA共享内存概述

GPU内存按照类型（物理上的位置）可以分为：

- 板载内存（容量高、延迟高、带宽低）
- 片上内存（容量小、延时低、带宽高）

共享内存是片上内存，与L1 cache共享片上的64KB的存储，共享内存可以看作是一种可编程、可控制的缓存，其主要作用有：

- 块内线程通信的通道
- 用于全局内存数据的缓存
- 用于转换数据、优化全局内存的访问模型

### 5.1.1 共享内存

共享内存（shared memory, SMEM）是 `GPU` 的关键部分，在物理层面，每个 `SM` 都有一个小的片上内存池，这个内存池被 `SM` 上活跃线程块上的所有线程共享。共享内存可以使同一线程块中可以相互协同，降低全局内存读取的延迟。

在GPU中，一级缓存、二级缓存以及共享内存、常量内存的关系如下图所示：

   ![GPU中的内存](/img/post_images/cuda/5-1.png)

在 `SM` 上有一级缓存、二级缓存、只读缓存以及常量缓存，所有从 `DRAM` 全局内存中过来的数据都要经过二级缓存，相比全局内存和二级缓存，SM上的内存延迟更低、带宽更高。

共享内存在所属的线程块被执行的时候建立，线程块执行完成后共享内存释放，线程块和它的共享内存有着相同的生命周期。

每个线程对于共享内存的访问请求，有三种情况：

1. 最好的情况是当前线程束中的每个线程都访问一个不冲突的共享内存地址，这种情况下一个事务就可以完成访问，效率最高。
2. 当访问有冲突的时候，最差情况是一个线程束需要32个内存事务。
3. 如果一个线程束中的32个线程访问同一个地址，那么一个线程访问完成以后会以广播的形式告诉其他线程。

### 5.1.2 共享内存分配

共享内存的定义包含两种：

- 全局共享内存（在所有的核函数外面定义，所有的核函数共享）
- 本地共享内存（在函数中定义，只能被当前的核函数访问）

共享内存的分配包括两种：

- 动态分配（在定义的时候不指定内存大小，调用核函数的时候指定，只能是一维的，定义时需要extern关键字）
- 静态分配（在定义的时候就指定内存大小，可以是一维、二维或者三维的）

共享内存的声明通过关键字 `__shared__` 指定，如：

```c
__shared__ float a[size_x][size_y];  // 声明一个静态共享内存
extern __shared__ int tile[]; // 声明一个动态共享内存
```

对于动态共享内存，在核函数调用的时候，需要在<<< >>>中将共享内存大小以第三个参数传入。

### 5.1.3 共享内存存储体和访问模式

优化内存性能主要有两个关键：

- 延迟
- 带宽

共享内存是用来隐藏全局内存延迟以及提高带宽性能的主要武器。

#### 5.1.3.1 内存存储体

共享内存是一个**一维的地址空间**，二维三维或者更多维的地址都要转换为一维的来应对物理上的内存地址。为了提高内存带宽，共享内存被分为32个同样大小的内存模型，成为存储体（bank），它们可以被同时访问。32个存储体对应的是一个线程束中有32个线程，如果一个线程束中的线程在访问共享内存的时候，都访问不同的存储体（无冲突），那么一个事务就能够完成，如果有冲突，则需要多个内存事务，这样就会导致带宽利用率降低。

#### 5.1.3.2 存储体冲突

多个线程访问一个存储体的时候会导致冲突（需要注意的是访问的是同一存储体中的不同地址，如果是同一个地址，则会通过广播形式，不会导致冲突）。线程束访问共享内存的时候有三种经典形式：

- 并行访问，多个地址访问多存储体
- 串行访问，多个地址访问同一存储体
- 广播访问，单一地址读取单一存储体

并行访问是最常见、效率最高的，可以分为完全无冲突和小部分冲突，完全无冲突是理想模式，这种情况下，线程束中所有线程对内存的访问通过一个事务完成，效率最高，小部分冲突时，不冲突的部分可以通过一个内存事务完成，冲突的部分被分割为由另外的不冲突的事务执行，会降低效率。

当小部分冲突编程完全冲突就是串行模式，这是最糟糕的形式，即所有的线程访问同一个存储体。

广播访问是所有的线程访问一个地址，这个时候，一个内存事务执行完毕后，一个线程得到了这个地址的数据，会通过广播的形式告诉其他所有的线程，虽然延迟相比与完全的并行访问并不慢，但是只读取了一个数据，会导致带宽利用率很差。

#### 5.1.3.3 访问模式

共享内存中，存储体的宽度随着设备计算能力的不同而变化，有以下两种情况：

- 2.x 计算能力的设备，为4字节，32位
- 3.x 计算能力的设备，为8字节，64位

即对于计算能力为 2.x 的设备来说，存储体索引与字节地址的对应关系为：

$$
indexOfBank = \frac{memoryAddress \div 4 }{numberOfBanks} \quad \% \quad numberOfBanks
$$

同一个线程束中的两个线程访问同一个地址不会发生冲突，会利用广播模式访问，但是对于写入，会造成不可预料的结果。

对于计算能力为 3.x 的设备来说，每个存储体的宽度变为 8 个字节，每个时钟周期内每个存储体都有64位的带宽，此时存储体冲突的概率会大大缩小。对于8个字节宽的存储体，有两种地址模式，

- 64 位模式
- 32 位模式

在64位模式中，连续的64位内存被映射到一个存储体中，如果两个线程同时访问同一个64位字中的任何子字，都不会产生冲突，因为满足这两个内存请求只需要一个64位的读操作。

在32位模式中，连续的32位内存被映射到一个存储体中，但是仍然保证每个存储体的位宽是64位。

#### 5.1.3.4 内存填充

存储体冲突会严重影响共享内存的效率，在遇到严重冲突的情况下，可以使用填充的办法让数据错误，降低冲突。

假设当前共有4个存储体，并声明一个二维共享内存 `__shared__ int a[5][4];`，如果线程束访问 bank0 中的不同数据，就会产生一个5向的冲突，此时如果将二维共享内存的声明修改为 `__shared__ int a[5][5];`，编译器就会将二维数组重新分配到存储体中，就会导致元素错开，从而避免冲突。需要注意的是，新增加的一列只是用来占位的，实际上存储数据，所以内存填充的方法会导致共享内存的浪费。

#### 5.1.3.5 访问模式配置

通过下面的API函数，可以查询共享内存访问模式是4字节还是8字节：

```c
cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig * pConfig);
// 其中 pConfig 可以是
// cudaSharedMemBankSizeFourByte 表示4字节访问
// cudaSharedMemBankSizeEightByte 表示8字节访问
```

在可配置的设备上，可以使用下面的函数来配置新的存储体大小：

```c
cudaError_t cudaDeviceSetShareMemConfig(cudaSharedMemConfig config);
// cudaSharedMemBankSizeDefault
// cudaSharedMemBankSizeFourByte
// cudaSharedMemBankSizeEightByte
```

修改存储体的大小不会增加共享内存的使用，也不会影响核函数的占用率，但是对核函数的性能有重大影响，大的存储体可能会有更高的带宽，但是也可能会导致更多的冲突，需要根据实际情况进行分析。

### 5.1.4 配置共享内存

每个 SM 上都有一个64KB的片上内存，共享内存和L1 cache 共享这64KB的内存，并且可以配置，可以通过两种方式配置L1 cache 和共享内存的大小：

- 按设备进行配置

```c
// 按照设备进行配置
cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig);
// 其中 cacheConfig 的值可以为：
// cudaFuncCachePreferNone: no preference(default)
// cudaFuncCachePreferShared: prefer 48KB shared memory and 16 KB L1 cache
// cudaFuncCachePreferL1: prefer 48KB L1 cache and 16 KB shared memory
// cudaFuncCachePreferEqual: prefer 32KB L1 cache and 32 KB shared memory
```

当共享内存使用较多时，优先配置更多的共享内存，当核函数需要使用更多的寄存器时，应优先配置更多的L1 cache。

- 按核函数进行配置

```c
cudaError_t cudaFuncSetCacheConfig(const void* func,
enum cudaFuncCacheca cheConfig);
// func 指的是核函数指针，cheConfig含义同上
```

### 5.1.5 同步

#### 5.1.5.1 弱排序内存模型

CUDA采用宽松的内存模型，也就是内存访问不一定按照他们在程序中出现的位置进行，即线程从不同内存中读取数据的顺序和读指令在程序中的顺序不一定相同，为了控制代码的运行，必须使用同步技术。

为了显示的强制程序以一个确切的顺序执行，必须在应用程序中插入内存栅栏和障碍点。

#### 5.1.5.2 显示障碍

使用下列函数，可以设置障碍点，这个函数只能在核函数中调用，且只对同一线程块中的线程有效，`__syncthreads` 保证同一线程块内的所有线程没有达到障碍点时，不能继续向下执行。

```c
void __syncthreads();
```

#### 5.1.5.3 内存栅栏

```c
void __threadfence_block(); // 线程块内
void __threadfence();   // 网格级内存栅栏
void __threadfence_system(); // 系统级栅栏
```

#### 5.1.5.4 Volatile修饰符

`volatile` 声明一个变量，防止编译器优化，防止这个变量存入缓存，如果恰好此时被其他线程改写，那就会造成内存缓存不一致的错误，所以 `volatile` 声明的变量始终在全局内存中

## 5.2 共享内存的数据布局

### 5.2.1 方形共享内存

对于一个定义好的二维共享内存：

```c
#define N 32
__shared__ int x[N][N];
```

可以用 $(y, x)$ 进行索引，也可以用 $(x, y)$ 进行索引，前者对应的是
 `int a = x[threadIdx.y][threadIdx.x]`，而后者对应的是 `int a = x[threadIdx.x][threadIdx.y]`。

CUDA中，一个线程束是沿着x方向切分的，也就是一个线程束中threadIdx.x是连续变化的，所以我们希望线程束中取数据是按照行来进行的，即按照 $(y, x)$ 的顺序进行索引，可以用一个内存事务完成所有的访问，而如果按照 $(x, y)$ 的顺序进行访问，会导致最大的bank冲突。

#### 5.2.1.1 行主序访问和列主序访问

对于如下的一个核函数，其按照行主序分别存储和读取数据:

```c
#define BDIMX 32
#define BDIMY 32

__global__ void setRowReadRow(int * out)
{
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;
    // 行主序存数据
    tile[threadIdx.y][threadIdx.x]=idx;
    __syncthreads();
    // 行主序读数据
    out[idx]=tile[threadIdx.y][threadIdx.x];
}
```

而对于下面的核函数，其按照列主序分别存储和读取数据

```c
__global__ void setColReadCol(int * out)
{
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;
    tile[threadIdx.x][threadIdx.y]=idx;
    __syncthreads();
    out[idx]=tile[threadIdx.x][threadIdx.y];
}
```

通过 `nvprof` 命令可以看到，行主序核函数的平均执行时间是 $1.552\mu s$，而列主序是 $2.4640\mu s$，通过 `nvprof` 可以检测bank冲突的指标：

```bash
# 共享内存加载数据内存消耗的事务数
nvprof  --metrics shared_load_transactions_per_request ./smem_ld
# 共享内存写入数据内存消耗的事务数
nvprof  --metrics shared_store_transactions_per_request ./smem_ld
```

结果显示，行主序的内存事务数是1，而列主序的内存事务是32。

#### 5.2.1.2 按行主序写和按列主序读

下面的核函数是行主序写、列主序读取的

```c
__global__ void setRowReadCol(int * out)
{
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;
    tile[threadIdx.y][threadIdx.x]=idx;
    __syncthreads();
    out[idx]=tile[threadIdx.x][threadIdx.y];
}
```

同上利用 `nvprof` 命令测试 `shared_load_transactions_per_request` 和  
`shared_store_transactions_per_request` 指标，共享内存读取操作是有32路冲突的，而写入操作没有冲突，只需要一个内存事务。

#### 5.2.1.3 动态共享内存

下列定义的核函数，使用了动态的共享内存：

```c
__global__ void setRowReadColDyn(int * out)
{
    extern __shared__ int tile[];
    unsigned int row_idx=threadIdx.y*blockDim.x+threadIdx.x;
    unsigned int col_idx=threadIdx.x*blockDim.y+threadIdx.y;
    tile[row_idx]=row_idx;
    __syncthreads();
    out[row_idx]=tile[col_idx];
}
```

需要注意的是，在定义动态共享内存的时候，需要使用 `extern` 关键字且动态共享内存是一维的，在核函数调用的时候，需要传入共享内存的大小：

```c
setRowReadColDyn<<<grid,block,BDIMX*BDIMY*sizeof(int)>>>(out);
```

使用共享内存定义的行主序写、列主序读运行结果与非动态内存的行主序写、列主序读结果没有什么差别，冲突不变。

#### 5.2.1.4 填充静态声明的共享内存

当我们声明一个二维或一维的共享内存时，编译器会首先把声明的共享内存转为一维线性的，再将其重新整理为二维按照32个存储体，4-Byte/8-Byte宽的内存分布。下面的核函数，通过对静态声明的共享内存进行填充，解决了bank冲突的问题。

```c
#define IPAD 1
__global__ void setRowReadColIpad(int * out)
{
    __shared__ int tile[BDIMY][BDIMX+IPAD];
    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;
    tile[threadIdx.y][threadIdx.x]=idx;
    __syncthreads();
    out[idx]=tile[threadIdx.x][threadIdx.y];
}
```

再次利用 `nvprof` 命令测试 `shared_load_transactions_per_request` 和  
`shared_store_transactions_per_request` 指标，上述的核函数所有的冲突都解决了，即加载和写入都只需要一个内存事务。

#### 5.2.1.5 填充动态声明的共享内存

下列的核函数对动态声明的共享内存进行填充：

```c
#define IPAD 1
__global__ void setRowReadColDynIpad(int * out)
{
    extern __shared__ int tile[];
    unsigned int row_idx=threadIdx.y*(blockDim.x+IPAD)+threadIdx.x;
    unsigned int col_idx=threadIdx.x*(blockDim.x+IPAD)+threadIdx.y; //?
    unsigned int g_idx = threadIdx.y*blockDim.x+threadIdx.x;
    tile[row_idx]=g_idx;
    __syncthreads();
    out[g_idx]=tile[col_idx];
}
```

利用 `nvprof` 命令测试 `shared_load_transactions_per_request` 和  
`shared_store_transactions_per_request` 指标，加载和写入都只需要一个内存事务。

#### 5.2.1.6 方形共享内存内核性能的比较

使用行主序写入、列主序读取，并增加填充的方式可以解决bank冲突问题。

### 5.2.2 矩形共享内存

对于长方形的共享内存，定义：

```c
#define BDIMX_RECT 32
#define BDIMY_RECT 16
```

在转换索引的时候，不能简单的交换坐标，而是先将索引换成一维线性的，再重新计算行和列的坐标。

#### 5.2.2.1 行主序访问与列主序访问

行主序访问代码：

```c
// 以下核函数在调用的时候，grid=(1, 1), block=(32, 16)
__global__ void setRowReadRow(int *out)
{
    // static shared memory
    __shared__ int tile[BDIMY_RECT][BDIMX_RECT];

    // mapping from thread index to global memory index
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;

    // shared memory store operation
    tile[threadIdx.y][threadIdx.x] = idx;

    // wait for all threads to complete
    __syncthreads();

    // shared memory load operation
    out[idx] = tile[threadIdx.y][threadIdx.x] ;
}
```

列主序访问代码：

```c
__global__ void setColReadCol(int *out)
{
    // static shared memory
    __shared__ int tile[BDIMY_RECT][BDIMX_RECT];

    // mapping from thread index to global memory index
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;
    int col_idx = idx / blockDim.y;
    int row_idx = idx % blockDim.y;

    // shared memory store operation
    tile[row_idx][col_idx] = idx;

    // wait for all threads to complete
    __syncthreads();

    // shared memory load operation
    out[idx] = tile[row_idx][col_idx];
}
```

此时，对行主序访问，没有bank冲突，而对于列主序访问，对于fermi架构有16向的bank冲突，对于kepler架构有8向的bank冲突。

#### 5.2.2.2 行主序写操作和列主序读操作

下列代码实现行主序写和列主序读操作：

```c
__global__ void setRowReadCol(int *out)
{
    // static shared memory
    __shared__ int tile[BDIMY_RECT][BDIMX_RECT];

    // mapping from thread index to global memory index
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;
    int row_idx = idx % blockDim.y;
    int col_idx = idx / blockDim.y;

    // shared memory store operation
    tile[threadIdx.y][threadIdx.x] = idx;

    // wait for all threads to complete
    __syncthreads();

    // shared memory load operation
    out[idx] = tile[row_idx][col_idx];
}

```

此核函数对于写访问不存在bank冲突，对于读访问，存在8路bank冲突（kepler架构）。

#### 5.2.2.3 动态声明的共享内存

动态共享内存只能被声明为一维的，将二维线程坐标转为一维的共享内存坐标需要引入一个新的索引：

```c
__global__ void setRowReadColDyn(int *out)
{
    // static shared memory
    __shared__ int tile[];

    // mapping from thread index to global memory index
    unsigned int idx = threadIdx.y * blockDim.x + threadIdx.x;
    int row_idx = idx % blockDim.y;
    int col_idx = idx / blockDim.y;

    // shared memory store operation
    tile[idx] = idx;

    // wait for all threads to complete
    __syncthreads();

    // 引入的新的索引
    int load_idx = row_idx * blockDim.x + col_idx;

    // shared memory load operation
    out[idx] = tile[load_idx];
}
```

此时核函数中的bank冲突与函数`setRowReadCol`一致。

#### 5.2.2.4 填充静态声明的共享内存

对于举行共享内存，也可以使用共享内存解决bank冲突，对于kepler架构，每一列需要填充的元素数目需要仔细计算。

```c
#define NPAD 2
__global__ void setRowReadColPad(int *out)
{
    // 定义共享内存
    __shared__ int tile[BDIMY_RECT][BDIMX_RECT+NPAD]；

    // 计算线程索引
    unsigned int idx = threadIdx.y*blockDim.x + threadIdx.x;

    // 将数据按行主序的顺序写入共享内存
    tile[threadIdx.y][threadIdx.x] = idx;

    // 计算全局内存索引
    unsigned int row_idx = idx % blockDim.y;
    unsigned int col_idx = idx / blockDim.y;

    __syncthreads();

    // 按照列主序的顺序读取共享内存
    out[idx] = tile[row_idx][col_idx];
}
```

对此核函数用nvporf检查内存事务，发现共享内存的读和写操作均只需要一次内存事务。

#### 5.2.2.5 填充动态声明的共享内存

对于动态共享内存，需要将二维的内存索引转换到一维索引，核函数如下：

```c
#define NPAD 2
__global__ void setRowReadColDynPad(int *out)
{
    //声明动态共享内存
    __shared__ int tile[];

    //计算线程索引
    unsigned int thread_idx = threadIdx.y * blockDim.x + threadIdx.x;

    //计算行主序写入共享内存的索引,并写入共享内存
    unsigned int set_idx = threadIdx.y * (blockDim.x+NPAD) + threadIdx.x;
    tile[set_idx] = thread_idx;

    //同步线程块
    __syncthreads();

    // 计算列主序读取共享内存的索引，并写到结果中
    unsigned int row_idx = thread_idx % blockDim.y;
    unsigned int col_idx = thread_idx / blockDim.y;
    unsigned int read_idx = row_idx * (blockDim.x+NPAD) + col_idx;

    out[idx] = tile[read_idx];
}
```

与填充静态共享内存一致，填充动态共享内存也能够解决列主序读取时的bank冲突。

## 5.3 减少全局内存访问

### 5.3.1 使用共享内存的并行归约

在全局内存下，完全展开的归约计算核函数如下：

```c
// unroll4 + complete unroll for loop + gmem
__global__ void reduceGmem(int *g_idata, int *g_odata, unsigned int n)
{
    // 线程在当前线程块中的索引
    unsigned int tid = threadIdx.x;
    // 计算当前线程块对应数据块的首地址
    int *idata = g_idata + blockIdx.x * blockDim.x;
    // 线程对应于全局内存数据的索引
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= n) return;

    // in-place reduction in global memory
    if (blockDim.x >= 1024 && tid < 512)
        idata[tid] += idata[tid + 512];
    __syncthreads();

    if (blockDim.x >= 512 && tid < 256)
        idata[tid] += idata[tid + 256];
    __syncthreads();

    if (blockDim.x >= 256 && tid < 128)
        idata[tid] += idata[tid + 128];
    __syncthreads();

    if (blockDim.x >= 128 && tid < 64) ]
        idata[tid] += idata[tid + 64];
    __syncthreads();

    // 将最后一个warp展开
    if (tid < 32)
    {
        volatile int *vsmem = idata;
        vsmem[tid] += vsmem[tid + 32];
        vsmem[tid] += vsmem[tid + 16];
        vsmem[tid] += vsmem[tid +  8];
        vsmem[tid] += vsmem[tid +  4];
        vsmem[tid] += vsmem[tid +  2];
        vsmem[tid] += vsmem[tid +  1];
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = idata[0];
}
```

下面是使用共享内存并行归约的核函数，与全局内存版本相比，知识多了一个共享内存声明，以及歌线程将全局内存中的数据写入到共享内存中，以及后面的同步指令：

```c
__global__ void reduceSmem(int *g_idata, int *g_odata, unsigned int n)
{
    __shared__ int smem[DIM];
    // set thread ID
    unsigned int tid = threadIdx.x;
    // boundary check
    unsigned int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx >= n) return;
    // convert global data pointer to the local pointer of this block
    int *idata = g_idata + blockIdx.x * blockDim.x;
    // set to smem by each threads
    smem[tid] = idata[tid];
    __syncthreads();

    // in-place reduction in shared memory
    if (blockDim.x >= 1024 && tid < 512)
        smem[tid] += smem[tid + 512];
    __syncthreads();

    if (blockDim.x >= 512 && tid < 256)
        smem[tid] += smem[tid + 256];
    __syncthreads();

    if (blockDim.x >= 256 && tid < 128)
        smem[tid] += smem[tid + 128];
    __syncthreads();

    if (blockDim.x >= 128 && tid < 64)
        smem[tid] += smem[tid + 64];
    __syncthreads();

    // unrolling warp
    if (tid < 32)
    {
        volatile int *vsmem = smem;
        vsmem[tid] += vsmem[tid + 32];
        vsmem[tid] += vsmem[tid + 16];
        vsmem[tid] += vsmem[tid +  8];
        vsmem[tid] += vsmem[tid +  4];
        vsmem[tid] += vsmem[tid +  2];
        vsmem[tid] += vsmem[tid +  1];
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = smem[0];
}
```

可以通过 `nvprof` 命令查看两个核函数运行的时间，以及查看 `gld_transactions` 和 `gst_transactions` 查看两个核函数全局内存事务数量，结论是后一个核函数的全局内存事务远远少与前一个只用全局内存版本的核函数。后者核函数运行的耗时也会远远少于前者。

### 5.3.2 使用展开的并行归约

在前面的核函数中，一个线程块处理一个数据块；第三章中提出，可以通过一次运行多个I/O操作，展开线程块来提升线程内核性能，通过展开线程块，可以：

- 通过在每个线程中提供更多的I/O操作，增加全局内存的吞吐量
- 减少全局内存存储事务
- 整体内核性能提升

使用共享内存展开的并行归约核函数如下：

```c
__global__ void reduceSmemUnroll(int *g_idata, int *g_odata, unsigned int n)
{
    // static shared memory
    __shared__ int smem[DIM];
    // set thread ID
    unsigned int tid = threadIdx.x;
    // global index, 4 blocks of input data processed at a time
    unsigned int idx = blockIdx.x * blockDim.x * 4 + threadIdx.x;
    // unrolling 4 blocks
    int tmpSum = 0;

    // boundary check
    if (idx < n)
    {
        int a1, a2, a3, a4;
        a1 = a2 = a3 = a4 = 0;
        a1 = g_idata[idx];
        if (idx + blockDim.x < n)
            a2 = g_idata[idx + blockDim.x];
        if (idx + 2 * blockDim.x < n)
            a3 = g_idata[idx + 2 * blockDim.x];
        if (idx + 3 * blockDim.x < n)
            a4 = g_idata[idx + 3 * blockDim.x];
        tmpSum = a1 + a2 + a3 + a4;
    }

    smem[tid] = tmpSum;
    __syncthreads();

    // in-place reduction in shared memory
    if (blockDim.x >= 1024 && tid < 512)
        smem[tid] += smem[tid + 512];
    __syncthreads();

    if (blockDim.x >= 512 && tid < 256)  
        smem[tid] += smem[tid + 256];
    __syncthreads();

    if (blockDim.x >= 256 && tid < 128)
        smem[tid] += smem[tid + 128];
    __syncthreads();

    if (blockDim.x >= 128 && tid < 64)
        smem[tid] += smem[tid + 64];
    __syncthreads();

    // unrolling warp
    if (tid < 32)
    {
        volatile int *vsmem = smem;
        vsmem[tid] += vsmem[tid + 32];
        vsmem[tid] += vsmem[tid + 16];
        vsmem[tid] += vsmem[tid +  8];
        vsmem[tid] += vsmem[tid +  4];
        vsmem[tid] += vsmem[tid +  2];
        vsmem[tid] += vsmem[tid +  1];
    }

    // write result for this block to global mem
    if (tid == 0) g_odata[blockIdx.x] = smem[0];
}
```

与展开之前相比，全局内存加载时的内存事务没有减少，但是全局内存存储时的内存事务大大减少。另外通过 `gld_throughput` 和 `gst_throughput` 可以查询全局内存加载和存储时的吞吐量，结论是加载吞吐量会增大，原因是有大量同时的加载请求，而存储时的吞吐量会降低，原因是较少的存储请求使总线达到了饱和。

### 5.3.3 使用动态共享内存时的并行归约

与 5.3.2 中的核函数差异不大。

### 5.3.4 有效带宽

归并核函数是受内存带宽约束的，评估内核函数的性能指标可以是有效带宽，有效带宽是指：核函数的完整执行时间内的I/O数量（以字节为单位），可以表示为：

$$
    effective bandwidth = (bytes read +  bytes written) \div
        (time elapsed  \times 10^-9) GB/s
$$

## 5.4 合并的全局内存访问

### 5.4.1 基准转置内核

对于矩阵转置的核函数，如果只使用全局内存，那么必然会导致读操作或者写操作用有一个不能合并访存，下面的核函数中，读内存是合并的，但是写内存访存是不合并的：

```c
__global__ void copyGmem(float *out, float *in, const int nrows, const int ncols)
{
    // matrix coordinate (ix,iy)
    unsigned int row = blockIdx.y * blockDim.y + threadIdx.y;
    unsigned int col = blockIdx.x * blockDim.x + threadIdx.x;

    // transpose with boundary test
    if (row < nrows && col < ncols)
    {
        // NOTE this is a transpose, not a copy
        out[col*nrows+row] = in[row*ncols+col];
    }
}
```

用 `nvprof` 命令检查加载和存储全局内存请求的平均事务的次数，（即 `gld_transactions_per_request` 和 `gst_transactions_per_request`），`gld_transactions_per_request` 的结果为1，而 `gst_transactions_per_request` 的结果是32。

### 5.4.2 使用共享内存的矩阵转置

为了避免交叉访问，可以使用二维共享内存缓存原始矩阵数据，然后从共享内存中读取一列存储到全局内存中，因为在共享内存中按列读取不会导致交叉访问那么严重的延迟，所以能在一定程度上提升效率。

下面是使用共享内存进行矩阵转置的核函数：

```c
#define BDIMX 16
#define BDIMY BDIMX
__global__ void transposeSmem(float *out, float *in, int nrows, int ncols)
{
    // 声明一段二维的矩形共享内存
    __shared__ float tile[BDIMY][BDIMX];

    // coordinate in original matrix
    unsigned int row = blockDim.y * blockIdx.y + threadIdx.y;
    unsigned int col = blockDim.x * blockIdx.x + threadIdx.x;

    // linear global memory index for original matrix
    unsigned int offset = row*ncols+col;

    if (row < nrows && col < ncols)
    {
      // load data from global memory to shared memory
      tile[threadIdx.y][threadIdx.x] = in[offset];
    }

    // thread index in transposed block
    unsigned int bidx, irow, icol;
    bidx = threadIdx.y * blockDim.x + threadIdx.x;
    irow = bidx / blockDim.y;
    icol = bidx % blockDim.y;

    // NOTE - need to transpose row and col on block and thread-block level:
    // 1. swap blocks x-y
    // 2. swap thread x-y assignment (irow and icol calculations above)
    // note col still has continuous threadIdx.x -> coalesced gst
    col = blockIdx.y * blockDim.y + icol;
    row = blockIdx.x * blockDim.x + irow;

    // linear global memory index for transposed matrix
    // NOTE nrows is stride of result, row and col are transposed
    unsigned int transposed_offset = row*nrows+col;
    // thread synchronization
    __syncthreads();

    // NOTE invert sizes for write check
    if (row < ncols && col < nrows)
    {
        // store data to global memory from shared memory
        out[transposed_offset] = tile[icol][irow]; // NOTE icol,irow not irow,icol
    }
}
```

对于上述的核函数，全局共享内存的加载和存储都是行主序的，也就是进行访存合并；而对于共享内存来说，数据写入是行主序的，没有bank冲突，但是对于数据读取，是列主序的，有8路的bank冲突（kepler架构），可以使用填充的方法解决冲突问题。

### 5.4.3 使用填充共享内存的矩阵转置

## 5.5 常量内存

## 5.6 线程束洗牌指令
