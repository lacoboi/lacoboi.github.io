---
layout:     post
title:      5.共享内存和常量内存
subtitle:   
date:       2019-08-10
author:     BY Lacoboi
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - CUDA
---

# 5.共享内存和常量内存

学习如何使用共享内存，提升核函数性能。

## 5.1 CUDA共享内存概述

GPU内存按照类型（物理上的位置）可以分为：

- 板载内存（容量高、延迟高、带宽低）
- 片上内存（容量小、延时低、带宽高）

共享内存是片上内存，与L1 cache共享片上的64KB的存储，共享内存可以看作是一种可编程、可控制的缓存，其主要作用有：

- 块内线程通信的通道
- 用于全局内存数据的缓存
- 用于转换数据、优化全局内存的访问模型

### 5.1.1 共享内存

共享内存（shared memory, SMEM）是 `GPU` 的关键部分，在物理层面，每个 `SM` 都有一个小的片上内存池，这个内存池被 `SM` 上活跃线程块上的所有线程共享。共享内存可以使同一线程块中可以相互协同，降低全局内存读取的延迟。

在GPU中，一级缓存、二级缓存以及共享内存、常量内存的关系如下图所示：

   ![GPU中的内存](/img/post_images/cuda/5-1.png)

在 `SM` 上有一级缓存、二级缓存、只读缓存以及常量缓存，所有从 `DRAM` 全局内存中过来的数据都要经过二级缓存，相比全局内存和二级缓存，SM上的内存延迟更低、带宽更高。

共享内存在所属的线程块被执行的时候建立，线程块执行完成后共享内存释放，线程块和它的共享内存有着相同的生命周期。

每个线程对于共享内存的访问请求，有三种情况：

1. 最好的情况是当前线程束中的每个线程都访问一个不冲突的共享内存地址，这种情况下一个事务就可以完成访问，效率最高。
2. 当访问有冲突的时候，最差情况是一个线程束需要32个内存事务。
3. 如果一个线程束中的32个线程访问同一个地址，那么一个线程访问完成以后会以广播的形式告诉其他线程。

### 5.1.2 共享内存分配

共享内存的定义包含两种：

- 全局共享内存（在所有的核函数外面定义，所有的核函数共享）
- 本地共享内存（在函数中定义，只能被当前的核函数访问）

共享内存的分配包括两种：

- 动态分配（在定义的时候不指定内存大小，调用核函数的时候指定，只能是一维的，定义时需要extern关键字）
- 静态分配（在定义的时候就指定内存大小，可以是一维、二维或者三维的）

共享内存的声明通过关键字 `__shared__` 指定，如：

```c
__shared__ float a[size_x][size_y];  // 声明一个静态共享内存
extern __shared__ int tile[]; // 声明一个动态共享内存
```

对于动态共享内存，在核函数调用的时候，需要在<<< >>>中将共享内存大小以第三个参数传入。

### 5.1.3 共享内存存储体和访问模式

优化内存性能主要有两个关键：

- 延迟
- 带宽

共享内存是用来隐藏全局内存延迟以及提高带宽性能的主要武器。

#### 5.1.3.1 内存存储体

共享内存是一个**一维的地址空间**，二维三维或者更多维的地址都要转换为一维的来应对物理上的内存地址。为了提高内存带宽，共享内存被分为32个同样大小的内存模型，成为存储体（bank），它们可以被同时访问。32个存储体对应的是一个线程束中有32个线程，如果一个线程束中的线程在访问共享内存的时候，都访问不同的存储体（无冲突），那么一个事务就能够完成，如果有冲突，则需要多个内存事务，这样就会导致带宽利用率降低。

#### 5.1.3.2 存储体冲突

多个线程访问一个存储体的时候会导致冲突（需要注意的是访问的是同一存储体中的不同地址，如果是同一个地址，则会通过广播形式，不会导致冲突）。线程束访问共享内存的时候有三种经典形式：

- 并行访问，多个地址访问多存储体
- 串行访问，多个地址访问同一存储体
- 广播访问，单一地址读取单一存储体

并行访问是最常见、效率最高的，可以分为完全无冲突和小部分冲突，完全无冲突是理想模式，这种情况下，线程束中所有线程对内存的访问通过一个事务完成，效率最高，小部分冲突时，不冲突的部分可以通过一个内存事务完成，冲突的部分被分割为由另外的不冲突的事务执行，会降低效率。

当小部分冲突编程完全冲突就是串行模式，这是最糟糕的形式，即所有的线程访问同一个存储体。

广播访问是所有的线程访问一个地址，这个时候，一个内存事务执行完毕后，一个线程得到了这个地址的数据，会通过广播的形式告诉其他所有的线程，虽然延迟相比与完全的并行访问并不慢，但是只读取了一个数据，会导致带宽利用率很差。

#### 5.1.3.3 访问模式

共享内存中，存储体的宽度随着设备计算能力的不同而变化，有以下两种情况：

- 2.x 计算能力的设备，为4字节，32位
- 3.x 计算能力的设备，为8字节，64位

即对于计算能力为 2.x 的设备来说，存储体索引与字节地址的对应关系为：

$$
indexOfBank = \frac{memoryAddress \div 4 }{numberOfBanks} \quad \% \quad numberOfBanks
$$

同一个线程束中的两个线程访问同一个地址不会发生冲突，会利用广播模式访问，但是对于写入，会造成不可预料的结果。

对于计算能力为 3.x 的设备来说，每个存储体的宽度变为 8 个字节，每个时钟周期内每个存储体都有64位的带宽，此时存储体冲突的概率会大大缩小。对于8个字节宽的存储体，有两种地址模式，

- 64 位模式
- 32 位模式

在64位模式中，连续的64位内存被映射到一个存储体中，如果两个线程同时访问同一个64位字中的任何子字，都不会产生冲突，因为满足这两个内存请求只需要一个64位的读操作。

在32位模式中，连续的32位内存被映射到一个存储体中，但是仍然保证每个存储体的位宽是64位。

#### 5.1.3.4 内存填充

存储体冲突会严重影响共享内存的效率，在遇到严重冲突的情况下，可以使用填充的办法让数据错误，降低冲突。

假设当前共有4个存储体，并声明一个二维共享内存 `__shared__ int a[5][4];`，如果线程束访问 bank0 中的不同数据，就会产生一个5向的冲突，此时如果将二维共享内存的声明修改为 `__shared__ int a[5][5];`，编译器就会将二维数组重新分配到存储体中，就会导致元素错开，从而避免冲突。需要注意的是，新增加的一列只是用来占位的，实际上存储数据，所以内存填充的方法会导致共享内存的浪费。

#### 5.1.3.5 访问模式配置

通过下面的API函数，可以查询共享内存访问模式是4字节还是8字节：

```c
cudaError_t cudaDeviceGetSharedMemConfig(cudaSharedMemConfig * pConfig);
// 其中 pConfig 可以是
// cudaSharedMemBankSizeFourByte 表示4字节访问
// cudaSharedMemBankSizeEightByte 表示8字节访问
```

在可配置的设备上，可以使用下面的函数来配置新的存储体大小：

```c
cudaError_t cudaDeviceSetShareMemConfig(cudaSharedMemConfig config);
// cudaSharedMemBankSizeDefault
// cudaSharedMemBankSizeFourByte
// cudaSharedMemBankSizeEightByte
```

修改存储体的大小不会增加共享内存的使用，也不会影响核函数的占用率，但是对核函数的性能有重大影响，大的存储体可能会有更高的带宽，但是也可能会导致更多的冲突，需要根据实际情况进行分析。

### 5.1.4 配置共享内存

每个 SM 上都有一个64KB的片上内存，共享内存和L1 cache 共享这64KB的内存，并且可以配置，可以通过两种方式配置L1 cache 和共享内存的大小：

- 按设备进行配置

```c
// 按照设备进行配置
cudaError_t cudaDeviceSetCacheConfig(cudaFuncCache cacheConfig);
// 其中 cacheConfig 的值可以为：
// cudaFuncCachePreferNone: no preference(default)
// cudaFuncCachePreferShared: prefer 48KB shared memory and 16 KB L1 cache
// cudaFuncCachePreferL1: prefer 48KB L1 cache and 16 KB shared memory
// cudaFuncCachePreferEqual: prefer 32KB L1 cache and 32 KB shared memory
```

当共享内存使用较多时，优先配置更多的共享内存，当核函数需要使用更多的寄存器时，应优先配置更多的L1 cache。

- 按核函数进行配置

```c
cudaError_t cudaFuncSetCacheConfig(const void* func,
enum cudaFuncCacheca cheConfig);
// func 指的是核函数指针，cheConfig含义同上
```

### 5.1.5 同步

#### 5.1.5.1 弱排序内存模型

CUDA采用宽松的内存模型，也就是内存访问不一定按照他们在程序中出现的位置进行，即线程从不同内存中读取数据的顺序和读指令在程序中的顺序不一定相同，为了控制代码的运行，必须使用同步技术。

为了显示的强制程序以一个确切的顺序执行，必须在应用程序中插入内存栅栏和障碍点。

#### 5.1.5.2 显示障碍

使用下列函数，可以设置障碍点，这个函数只能在核函数中调用，且只对同一线程块中的线程有效，`__syncthreads` 保证同一线程块内的所有线程没有达到障碍点时，不能继续向下执行。

```c
void __syncthreads();
```

#### 5.1.5.3 内存栅栏

```c
void __threadfence_block(); // 线程块内
void __threadfence();   // 网格级内存栅栏
void __threadfence_system(); // 系统级栅栏
```

#### 5.1.5.4 Volatile修饰符

`volatile` 声明一个变量，防止编译器优化，防止这个变量存入缓存，如果恰好此时被其他线程改写，那就会造成内存缓存不一致的错误，所以 `volatile` 声明的变量始终在全局内存中

## 5.2 共享内存的数据布局

### 5.2.1 方形共享内存

对于一个定义好的二维共享内存：

```c
#define N 32
__shared__ int x[N][N];
```

可以用 $(y, x)$ 进行索引，也可以用 $(x, y)$ 进行索引，前者对应的是
 `int a = x[threadIdx.y][threadIdx.x]`，而后者对应的是 `int a = x[threadIdx.x][threadIdx.y]`。

CUDA中，一个线程束是沿着x方向切分的，也就是一个线程束中threadIdx.x是连续变化的，所以我们希望线程束中取数据是按照行来进行的，即按照 $(y, x)$ 的顺序进行索引，可以用一个内存事务完成所有的访问，而如果按照 $(x, y)$ 的顺序进行访问，会导致最大的bank冲突。

#### 5.2.1.1 行主序访问和列主序访问

对于如下的一个核函数，其按照行主序分别存储和读取数据:

```c
#define BDIMX 32
#define BDIMY 32

__global__ void setRowReadRow(int * out)
{
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;
    // 行主序存数据
    tile[threadIdx.y][threadIdx.x]=idx;
    __syncthreads();
    // 行主序读数据
    out[idx]=tile[threadIdx.y][threadIdx.x];
}
```

而对于下面的核函数，其按照列主序分别存储和读取数据

```c
__global__ void setColReadCol(int * out)
{
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;
    tile[threadIdx.x][threadIdx.y]=idx;
    __syncthreads();
    out[idx]=tile[threadIdx.x][threadIdx.y];
}
```

通过 `nvprof` 命令可以看到，行主序核函数的平均执行时间是 $1.552\mu s$，而列主序是 $2.4640\mu s$，通过 `nvprof` 可以检测bank冲突的指标：

```bash
# 共享内存加载数据内存消耗的事务数
nvprof  --metrics shared_load_transactions_per_request ./smem_ld
# 共享内存写入数据内存消耗的事务数
nvprof  --metrics shared_store_transactions_per_request ./smem_ld
```

结果显示，行主序的内存事务数是1，而列主序的内存事务是32。

#### 5.2.1.2 按行主序写和按列主序读

下面的核函数是行主序写、列主序读取的

```c
__global__ void setRowReadCol(int * out)
{
    __shared__ int tile[BDIMY][BDIMX];
    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;
    tile[threadIdx.y][threadIdx.x]=idx;
    __syncthreads();
    out[idx]=tile[threadIdx.x][threadIdx.y];
}
```

同上利用 `nvprof` 命令测试 `shared_load_transactions_per_request` 和  
`shared_store_transactions_per_request` 指标，共享内存读取操作是有32路冲突的，而写入操作没有冲突，只需要一个内存事务。

#### 5.2.1.3 动态共享内存

下列定义的核函数，使用了动态的共享内存：

```c
__global__ void setRowReadColDyn(int * out)
{
    extern __shared__ int tile[];
    unsigned int row_idx=threadIdx.y*blockDim.x+threadIdx.x;
    unsigned int col_idx=threadIdx.x*blockDim.y+threadIdx.y;
    tile[row_idx]=row_idx;
    __syncthreads();
    out[row_idx]=tile[col_idx];
}
```

需要注意的是，在定义动态共享内存的时候，需要使用 `extern` 关键字且动态共享内存是一维的，在核函数调用的时候，需要传入共享内存的大小：

```c
setRowReadColDyn<<<grid,block,BDIMX*BDIMY*sizeof(int)>>>(out);
```

使用共享内存定义的行主序写、列主序读运行结果与非动态内存的行主序写、列主序读结果没有什么差别，冲突不变。

#### 5.2.1.4 填充静态声明的共享内存

当我们声明一个二维或一维的共享内存时，编译器会首先把声明的共享内存转为一维线性的，再将其重新整理为二维按照32个存储体，4-Byte/8-Byte宽的内存分布。下面的核函数，通过对静态声明的共享内存进行填充，解决了bank冲突的问题。

```c
#define IPAD 1
__global__ void setRowReadColIpad(int * out)
{
    __shared__ int tile[BDIMY][BDIMX+IPAD];
    unsigned int idx=threadIdx.y*blockDim.x+threadIdx.x;
    tile[threadIdx.y][threadIdx.x]=idx;
    __syncthreads();
    out[idx]=tile[threadIdx.x][threadIdx.y];
}
```

再次利用 `nvprof` 命令测试 `shared_load_transactions_per_request` 和  
`shared_store_transactions_per_request` 指标，上述的核函数所有的冲突都解决了，即加载和写入都只需要一个内存事务。

#### 5.2.1.5 填充动态声明的共享内存

下列的核函数对动态声明的共享内存进行填充：

```c
#define IPAD 1
__global__ void setRowReadColDynIpad(int * out)
{
    extern __shared__ int tile[];
    unsigned int row_idx=threadIdx.y*(blockDim.x+IPAD)+threadIdx.x;
    unsigned int col_idx=threadIdx.x*(blockDim.x+IPAD)+threadIdx.y; //?
    unsigned int g_idx = threadIdx.y*blockDim.x+threadIdx.x;
    tile[row_idx]=g_idx;
    __syncthreads();
    out[g_idx]=tile[col_idx];
}
```

利用 `nvprof` 命令测试 `shared_load_transactions_per_request` 和  
`shared_store_transactions_per_request` 指标，加载和写入都只需要一个内存事务。

#### 5.2.1.6 方形共享内存内核性能的比较

使用行主序写入、列主序读取，并增加填充的方式可以解决bank冲突问题。

### 5.2.2 矩形共享内存

对于长方形的共享内存，定义：

```c
#define BDIMX_RECT 32
#define BDIMY_RECT 16
```

在转换索引的时候，不能简单的交换坐标，而是先将索引换成一维线性的，再重新计算行和列的坐标。

### 5.2.2.1 行主序访问和列主序访问

## 5.3 减少全局内存访问

## 5.4 合并的全局内存访问

## 5.5 常量内存

## 5.6 线程束洗牌指令
