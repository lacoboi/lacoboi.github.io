---
layout:     post
title:      CV中注意力机制（3）
subtitle:   
date:       2020-02-27
author:     BY Lacoboi
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - CV
    - attention
---

## Non-local

`Non-Local` 是 `CVPR2018` 年提出的一个自注意力模型，`Non-Local` 可以让感受野很大，而不是局限在一个局部邻域，其优点是：

- 提出的 `non-local operations` 通过计算任意两个位置之间的交互直接捕捉远程依赖，而不用局限于相邻点，其相当于构造了一个和特征图谱尺寸一样大的卷积核, 从而可以维持更多信息。

`Non-Local` 的通用公式表示为：

$$
y_i = \frac{1}{C(x)} \sum_{\forall j} f(x_i, x_j) g(x_j)
$$

其中：

- $x$ 表示输入信号，cv中一般表示feature map;
- $i$ 代表的是输出位置，如空间、时间或者时空的索引，他的响应应该对 $j$ 进行枚举然后计算得到的;
- $f$ 函数计算 $i$ 和 $j$ 的相似度；
- $g$ 函数表示 feature map 在 $j$ 位置的表示，$g$ 函数可以看成是一个线性转换，可以通过一个 1x1 的卷积实现；
- 最终的 $y$ 是通过响应因子 $C(x)$ 进行标准化处理以后得到的；

从实现角度，上述公式进行理解如下，$x$ 是输入的featur map，形状为 [c, h, w],经过三个 1x1 的卷积核，将通道缩减为原来的一半（c/2）；然后将h，w两个维度进行flatten，变为hxw，最终形状为 [c / 2, h * w] 的 Tensor，对 $\theta$ 对应的通道进行重排，也就是转置为 [h * w, c/2] 的tensor，然后将其与 $\phi$ 对应的 tensor 进行相乘， 得到的是形状为 [w * h， w * h]的矩阵，该矩阵表示相似度（或者理解为attention），然后经过 softmax 进行归一化，然后将该相似度矩阵与函数 $g$ 对应的矩阵进行相乘，得到[h * w, c/2]的矩阵，最后将该矩阵进行解flatten，并用1x1的卷积核将通道拓展，重新得到一个 [c, h, w] 的矩阵：

   ![NL](/img/post_images/attention/non-local.jpeg)

## Non-local 代码实现

```python
import torch
from torch import nn
from torch.nn import functional as F

class _NonLocalBlockND(nn.Module):
    """
    调用过程 NONLocalBlock2D(in_channels=32)
    """
    def __init__(self,
                 in_channels,
                 inter_channels=None,
                 dimension=3,
                 sub_sample=True,
                 bn_layer=True):
        super(_NonLocalBlockND, self).__init__()

        assert dimension in [1, 2, 3]
        self.dimension = dimension
        self.sub_sample = sub_sample
        self.in_channels = in_channels
        self.inter_channels = inter_channels

        if self.inter_channels isNone:
            self.inter_channels = in_channels // 2
            # 进行压缩得到channel个数

            if self.inter_channels == 0:
                self.inter_channels = 1

        if dimension == 3:
            conv_nd = nn.Conv3d
            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))
            bn = nn.BatchNorm3d
        elif dimension == 2:
            conv_nd = nn.Conv2d
            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))
            bn = nn.BatchNorm2d
        else:
            conv_nd = nn.Conv1d
            max_pool_layer = nn.MaxPool1d(kernel_size=(2))
            bn = nn.BatchNorm1d

        self.g = conv_nd(in_channels=self.in_channels,
                         out_channels=self.inter_channels,
                         kernel_size=1,
                         stride=1,
                         padding=0)

        if bn_layer:
            self.W = nn.Sequential(
                conv_nd(in_channels=self.inter_channels,
                        out_channels=self.in_channels,
                        kernel_size=1,
                        stride=1,
                        padding=0), bn(self.in_channels))
            nn.init.constant_(self.W[1].weight, 0)
            nn.init.constant_(self.W[1].bias, 0)
        else:
            self.W = conv_nd(in_channels=self.inter_channels,
                             out_channels=self.in_channels,
                             kernel_size=1,
                             stride=1,
                             padding=0)
            nn.init.constant_(self.W.weight, 0)
            nn.init.constant_(self.W.bias, 0)

        self.theta = conv_nd(in_channels=self.in_channels,
                             out_channels=self.inter_channels,
                             kernel_size=1,
                             stride=1,
                             padding=0)
        self.phi = conv_nd(in_channels=self.in_channels,
                           out_channels=self.inter_channels,
                           kernel_size=1,
                           stride=1,
                           padding=0)

        if sub_sample:
            self.g = nn.Sequential(self.g, max_pool_layer)
            self.phi = nn.Sequential(self.phi, max_pool_layer)

    def forward(self, x):
        '''
        :param x: (b, c,  h, w)
        :return:
        '''

        batch_size = x.size(0)
        g_x = self.g(x).view(batch_size, self.inter_channels, -1) #[bs, c, w*h]

        g_x = g_x.permute(0, 2, 1)
        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)
        theta_x = theta_x.permute(0, 2, 1)
        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)
        f = torch.matmul(theta_x, phi_x)
        print(f.shape)
        f_div_C = F.softmax(f, dim=-1)
        y = torch.matmul(f_div_C, g_x)
        y = y.permute(0, 2, 1).contiguous()
        y = y.view(batch_size, self.inter_channels, *x.size()[2:])
        W_y = self.W(y)
        z = W_y + x
        return z
```

论文中给出了4个相似度计算的模型，但是效果相差不大。

## GCNet

`GCNet` 论文名称是 `GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond`，是 `SENet` 和 `Non Local block` 的融合，克服了 `Non local` 计算量过大的缺点。

论文中首先对 `Non-local` 进行实验，对于不同的查询分别对 Attention maps进行可视化，分析发现，对于不同的查询点，其attention map几乎是一致的，这说明 `Non-local` 学习到的 attention map是有冗余的，基于以上发现，论文中首先提出了可以结合 SENET 和 Non-local的优点，并提出了 `Simplified NLNet`，极大的减少了计算量。

![simpleNL](/img/post_images/attention/simpleNL.webp)

简化后的 NLnet 虽然计算量减少了，但是准确率并没有提升，论文中又将 `SENet` 与 `simple Non-local` 结合，提出了 `GCNet`。

![simpleNL2](/img/post_images/attention/simpleNL2.webp)

可以看出，`GCNet` 在上下文信息建模这个地方使用了 `Simplified NL block` 中的机制，可以充分利用全局上下文信息， 同时在Transform阶段借鉴了SE block，GCNet使用的的位置实在每个Stage之间的连接部分。

## GCNet 实现代码

```python
import torch
from torch import nn

class ContextBlock(nn.Module):
    def __init__(self,inplanes,ratio,pooling_type='att',
                 fusion_types=('channel_add', )):
        super(ContextBlock, self).__init__()
        valid_fusion_types = ['channel_add', 'channel_mul']

        assert pooling_type in ['avg', 'att']
        assert isinstance(fusion_types, (list, tuple))
        assert all([f in valid_fusion_types for f in fusion_types])
        assert len(fusion_types) > 0, 'at least one fusion should be used'

        self.inplanes = inplanes
        self.ratio = ratio
        self.planes = int(inplanes * ratio)
        self.pooling_type = pooling_type
        self.fusion_types = fusion_types

        if pooling_type == 'att':
            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)
            self.softmax = nn.Softmax(dim=2)
        else:
            self.avg_pool = nn.AdaptiveAvgPool2d(1)
        if'channel_add'in fusion_types:
            self.channel_add_conv = nn.Sequential(
                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),
                nn.LayerNorm([self.planes, 1, 1]),
                nn.ReLU(inplace=True),  # yapf: disable
                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))
        else:
            self.channel_add_conv = None
        if'channel_mul'in fusion_types:
            self.channel_mul_conv = nn.Sequential(
                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),
                nn.LayerNorm([self.planes, 1, 1]),
                nn.ReLU(inplace=True),  # yapf: disable
                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))
        else:
            self.channel_mul_conv = None


    def spatial_pool(self, x):
        batch, channel, height, width = x.size()
        if self.pooling_type == 'att':
            input_x = x
            # [N, C, H * W]
            input_x = input_x.view(batch, channel, height * width)
            # [N, 1, C, H * W]
            input_x = input_x.unsqueeze(1)
            # [N, 1, H, W]
            context_mask = self.conv_mask(x)
            # [N, 1, H * W]
            context_mask = context_mask.view(batch, 1, height * width)
            # [N, 1, H * W]
            context_mask = self.softmax(context_mask)
            # [N, 1, H * W, 1]
            context_mask = context_mask.unsqueeze(-1)
            # [N, 1, C, 1]
            context = torch.matmul(input_x, context_mask)
            # [N, C, 1, 1]
            context = context.view(batch, channel, 1, 1)
        else:
            # [N, C, 1, 1]
            context = self.avg_pool(x)
        return context

    def forward(self, x):
        # [N, C, 1, 1]
        context = self.spatial_pool(x)
        out = x
        if self.channel_mul_conv isnotNone:
            # [N, C, 1, 1]
            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))
            out = out * channel_mul_term
        if self.channel_add_conv isnotNone:
            # [N, C, 1, 1]
            channel_add_term = self.channel_add_conv(context)
            out = out + channel_add_term
        return out

if __name__ == "__main__":
    in_tensor = torch.ones((12, 64, 128, 128))
    cb = ContextBlock(inplanes=64, ratio=1./16.,pooling_type='att')
    out_tensor = cb(in_tensor)
    print(in_tensor.shape)
    print(out_tensor.shape)
```
