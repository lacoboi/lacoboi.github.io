---
layout:     post
title:      CV中注意力机制（2）
subtitle:   
date:       2020-02-19
author:     BY Lacoboi
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - CV
    - attention
---

# CV中注意力机制（2）

## CBAM模块

注意力机制就是希望网络能够自动学习图片或者文本中需要注意的地方，从实现角度来看，注意力机制就是生成一个掩码Mask，mask上的值会对特征进行打分。

注意力机制可以分为：

- **通道注意力机制**，对通道生成Mask，代表是SENet，SKNet
- **空间注意力机制**，对空间进行掩码生成Mask，代表是 Spatial Attention Module
- **混合注意力机制**，同时生成通道注意力和空间注意力，代表有BAM，CBAM

`CBAM` 的全称是 `Convolutional Block Attention Module`，其网络架构非常简单，先后集成了通道注意力机制和空间注意力机制。

### 通道注意力机制

`CBAM` 模块中的通道注意力机制如下图所示，对于输入的特征 `F`，分别进行 `MaxPool` 和 `AvgPool` 操作，然后通过一个共享的 MLP模块进行非线性变换，得到的结果再进行叠加和sigmoid，得到 `1x1xC` 的一维特征向量。

   ![CBAM01](/img/post_images/attention/cbam01.webp)

通道注意力机制实现代码如下：

```python
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, rotio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        self.sharedMLP = nn.Sequential(
            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_planes // rotio, in_planes, 1, bias=False))
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = self.sharedMLP(self.avg_pool(x))
        maxout = self.sharedMLP(self.max_pool(x))
        return self.sigmoid(avgout + maxout)
```

### 空间注意力机制

`CBAM` 模块中的空间注意力机制如下图所示，同样做 `MaxPool` 和 `AvgPool`，只是与通道注意力机制不同的是，得到的是两个 `W x H` 大小的特征，将两个特征图进行concat叠加，再利用一个`3x3`的卷积层进行通道压缩得到`1xWxH`的空间注意力机制特征，最后进行一个sigmoid进行非线性变换。

   ![CBAM02](/img/post_images/attention/cbam02.png)

实现代码如下：

```python
class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        assert kernel_size in (3,7), "kernel size must be 3 or 7"
        padding = 3 if kernel_size == 7 else 1

        self.conv = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avgout = torch.mean(x, dim=1, keepdim=True)
        maxout, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avgout, maxout], dim=1)
        x = self.conv(x)
        return self.sigmoid(x)
```

### Convolutional bottleneck attention module

`CBAM` 模块中先后集成了通道注意力机制和空间注意力机制，如下图所示：

   ![CBAM03](/img/post_images/attention/cbam03.png)

## BAM模块

上面的 `CBAM` 模块可以看做是通道注意力和空间注意力的串联（先通道注意力，再空间注意力），而`BAM`模块可以看作是两者的并联。

   ![BAM02](/img/post_images/attention/bam02.webp)

`BAM` 是 `bottlenect attention module` 的缩写，之所以叫 `bottlenect` 是因为这个模块放在 `DownSample` 也就是 `pooling layer` 之前，如下图所示:

   ![BAM01](/img/post_images/attention/bam01.webp)

### 代码实现

与 `CBAM` 相似， `BAM` 的实现也可以分为通道注意力实现和空间注意力实现，然后将二者得到的注意力进行叠加。通道注意力的实现代码如下所示：

```python
class Flatten(nn.Module):
    def forward(self, x):
        return x.view(x.size(0), -1)

class ChannelGate(nn.Module):
    def __init__(self, gate_channel, reduction_ratio=16, num_layers=1):
        super(ChannelGate, self).__init__()
        self.gate_c = nn.Sequential()
        self.gate_c.add_module('flatten', Flatten())

        gate_channels = [gate_channel]  # eg 64

        gate_channels += [gate_channel // reduction_ratio] * num_layers  # eg 4

        gate_channels += [gate_channel]  # 64
        # gate_channels: [64, 4, 64]

        for i in range(len(gate_channels) - 2):
            self.gate_c.add_module(
                'gate_c_fc_%d' % i,
                nn.Linear(gate_channels[i], gate_channels[i + 1]))
            self.gate_c.add_module('gate_c_bn_%d' % (i + 1),
                                   nn.BatchNorm1d(gate_channels[i + 1]))
            self.gate_c.add_module('gate_c_relu_%d' % (i + 1), nn.ReLU())

        self.gate_c.add_module('gate_c_fc_final',
                               nn.Linear(gate_channels[-2], gate_channels[-1]))

    def forward(self, x):
        avg_pool = F.avg_pool2d(x, x.size(2), stride=x.size(2))
        return self.gate_c(avg_pool).unsqueeze(2).unsqueeze(3).expand_as(x)
```

其中，`avg_pool2d` 的作用与 `AdaptiveAvgPool2d(1)`的结果是一致的。

```python
>>> import torch.nn.functional as F
>>> import torch
>>> x = torch.ones((12, 8, 64, 64))
>>> x.shape
torch.Size([12, 8, 64, 64])
>>> F.avg_pool2d(x,x.size(2), stride=x.size(2)).shape
torch.Size([12, 8, 1, 1])
```

空间注意力机制的实现代码如下：

```python
class SpatialGate(nn.Module):
    def __init__(self,
                 gate_channel,
                 reduction_ratio=16,
                 dilation_conv_num=2,
                 dilation_val=4):
        super(SpatialGate, self).__init__()
        self.gate_s = nn.Sequential()

        self.gate_s.add_module(
            'gate_s_conv_reduce0',
            nn.Conv2d(gate_channel,
                      gate_channel // reduction_ratio,
                      kernel_size=1))
        self.gate_s.add_module('gate_s_bn_reduce0',
                               nn.BatchNorm2d(gate_channel // reduction_ratio))
        self.gate_s.add_module('gate_s_relu_reduce0', nn.ReLU())

        # 进行多个空洞卷积，丰富感受野
        for i in range(dilation_conv_num):
            self.gate_s.add_module(
                'gate_s_conv_di_%d' % i,
                nn.Conv2d(gate_channel // reduction_ratio,
                          gate_channel // reduction_ratio,
                          kernel_size=3,
                          padding=dilation_val,
                          dilation=dilation_val))
            self.gate_s.add_module(
                'gate_s_bn_di_%d' % i,
                nn.BatchNorm2d(gate_channel // reduction_ratio))
            self.gate_s.add_module('gate_s_relu_di_%d' % i, nn.ReLU())

        self.gate_s.add_module(
            'gate_s_conv_final',
            nn.Conv2d(gate_channel // reduction_ratio, 1, kernel_size=1))

    def forward(self, x):
        return self.gate_s(x).expand_as(x)
```

上述代码的大致流程如下：

- 首先经过一个 conv + bn + relu 模块，进行通道压缩；
- 经过多个 dilated conv + bn + relu 模块，空洞率默认为 4；
- 最后经过一个卷积，将通道压缩到1；

## 总结对比

`CBAM` 与 `BAM` 都是混合了通道注意力机制与空间注意力机制的attention模块，但是其实现有略微的差异，从整体效果来看，`CBAM` 效果略好一些。
